{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MIT License\n",
    "Copyright (c) 2023 Okyaz Eminaga\n",
    "Permission is hereby granted, free of charge, to any person obtaining a copy\n",
    "of this software and associated documentation files (the \"Software\"), to deal\n",
    "in the Software without restriction, including without limitation the rights\n",
    "to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n",
    "copies of the Software, and to permit persons to whom the Software is\n",
    "furnished to do so, subject to the following conditions:\n",
    "The above copyright notice and this permission notice shall be included in all\n",
    "copies or substantial portions of the Software.\n",
    "THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n",
    "IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n",
    "FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n",
    "AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n",
    "LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n",
    "OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n",
    "SOFTWARE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lifelines.utils.concordance import concordance_index\n",
    "from pathlib import Path\n",
    "import tensorflow as tf\n",
    "from tensorflow.compat.v1.keras.backend import set_session\n",
    "from typing import Dict, Iterable, Sequence, Tuple, Optional\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "\n",
    "\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "\n",
    "\n",
    "config = tf.compat.v1.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "set_session(tf.compat.v1.Session(config=config))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from albumentations import (\n",
    "    Compose, RandomBrightness, JpegCompression, HueSaturationValue, RandomContrast, HorizontalFlip,\n",
    "    Rotate, RandomSizedCrop, CenterCrop\n",
    ")\n",
    "transforms = Compose([\n",
    "    Rotate(limit=40),\n",
    "    RandomBrightness(limit=0.1),\n",
    "    JpegCompression(quality_lower=85, quality_upper=100, p=0.5),\n",
    "    HueSaturationValue(hue_shift_limit=20, sat_shift_limit=30,\n",
    "                       val_shift_limit=20, p=0.5),\n",
    "    RandomContrast(limit=0.2, p=0.5),\n",
    "    HorizontalFlip()\n",
    "])\n",
    "\n",
    "no_change_transform = Compose([CenterCrop(4096, 4096, always_apply=True), RandomSizedCrop(\n",
    "    [512, 586], 512, 512, p=1.0, always_apply=True)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "\n",
    "\n",
    "def _make_riskset(time: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"Compute mask that represents each sample's risk set.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    time : np.ndarray, shape=(n_samples,)\n",
    "        Observed event time sorted in descending order.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    risk_set : np.ndarray, shape=(n_samples, n_samples)\n",
    "        Boolean matrix where the `i`-th row denotes the\n",
    "        risk set of the `i`-th instance, i.e. the indices `j`\n",
    "        for which the observer time `y_j >= y_i`.\n",
    "    \"\"\"\n",
    "    assert time.ndim == 1, \"expected 1D array\"\n",
    "\n",
    "    # sort in descending order\n",
    "    o = np.argsort(-time, kind=\"mergesort\")\n",
    "    n_samples = len(time)\n",
    "    risk_set = np.zeros((n_samples, n_samples), dtype=np.bool_)\n",
    "    for i_org, i_sort in enumerate(o):\n",
    "        ti = time[i_sort]\n",
    "        k = i_org\n",
    "        while k < n_samples and ti == time[o[k]]:\n",
    "            k += 1\n",
    "        risk_set[i_sort, o[:k]] = True\n",
    "    return risk_set\n",
    "\n",
    "\n",
    "class InputFunction(object):\n",
    "    \"\"\"Callable input function that computes the risk set for each batch.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    images : np.ndarray, shape=(n_samples, height, width)\n",
    "        Image data.\n",
    "    time : np.ndarray, shape=(n_samples,)\n",
    "        Observed time.\n",
    "    event : np.ndarray, shape=(n_samples,)\n",
    "        Event indicator.\n",
    "    batch_size : int, optional, default=64\n",
    "        Number of samples per batch.\n",
    "    drop_last : int, optional, default=False\n",
    "        Whether to drop the last incomplete batch.\n",
    "    shuffle : bool, optional, default=False\n",
    "        Whether to shuffle data.\n",
    "    seed : int, optional, default=89\n",
    "        Random number seed.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 x: np.ndarray,\n",
    "                 time: np.ndarray,\n",
    "                 event: np.ndarray,\n",
    "                 augmentation: bool = False,\n",
    "                 input_size: (int, int) = (512, 512),\n",
    "                 channel_number: int = 3,\n",
    "                 batch_size: int = 32,\n",
    "                 drop_last: bool = False,\n",
    "                 shuffle: bool = False,\n",
    "                 k: int = 1,\n",
    "                 read_file: bool = False,\n",
    "                 repeat: int = 1,\n",
    "                 resize_img: bool = False,\n",
    "                 seed: int = 89) -> None:\n",
    "        self.x = x\n",
    "        self.time = time\n",
    "        self.input_size = input_size\n",
    "        self.augmentation = augmentation\n",
    "        self.event = event\n",
    "        self.batch_size = batch_size\n",
    "        self.drop_last = drop_last\n",
    "        self.shuffle = shuffle\n",
    "        self.seed = seed\n",
    "        self.repeat = repeat\n",
    "        self.k = k\n",
    "        self.resize_img = resize_img\n",
    "        self.read_file = read_file\n",
    "        self.channel_number = channel_number\n",
    "\n",
    "    def size(self) -> int:\n",
    "        \"\"\"Total number of samples.\"\"\"\n",
    "        return len(self.x)\n",
    "\n",
    "    def steps_per_epoch(self) -> int:\n",
    "        \"\"\"Number of batches for one epoch.\"\"\"\n",
    "        return int(np.floor(len(self.x) / self.batch_size))\n",
    "\n",
    "    def _get_data_batch(self, index: np.ndarray) -> Tuple[np.ndarray, Dict[str, np.ndarray]]:\n",
    "        \"\"\"Compute risk set for samples in batch.\"\"\"\n",
    "        time = self.time[index].copy()\n",
    "        event = self.event[index].copy()\n",
    "        x = self.x[index].copy()\n",
    "        if self.read_file:\n",
    "            images = []\n",
    "            for fl in x:\n",
    "                img = Image.open(fl)\n",
    "                img = img.resize((5120//self.k, 5120//self.k))\n",
    "                img = np.array(img)\n",
    "                # img = cv2.resize(img, (5120//self.k, 5120//self.k))\n",
    "                data = {\"image\": img}\n",
    "                if self.augmentation:\n",
    "                    aug_data = transforms(**data)\n",
    "                else:\n",
    "                    aug_data = no_change_transform(**data)\n",
    "                aug_img = aug_data[\"image\"]\n",
    "                images.append(aug_img)\n",
    "            x = np.array(images)\n",
    "        else:\n",
    "            if self.resize_img:\n",
    "                x_tmp = []\n",
    "                for j in range(x.shape[0]):\n",
    "                    x_tmp.append(\n",
    "                        resize(x[j], self.input_size, preserve_range=True).astype(np.uint8))\n",
    "                x = np.array(x_tmp)\n",
    "            if self.augmentation:\n",
    "                for i in range(x.shape[0]):\n",
    "                    data = {\"image\": x[i]}\n",
    "                    aug_data = transforms(**data)\n",
    "                    x[i] = aug_data[\"image\"]\n",
    "\n",
    "        labels = {\n",
    "            \"label_event\": event.astype(np.int32),\n",
    "            \"label_time\": time.astype(np.float32),\n",
    "            \"label_riskset\": _make_riskset(time)\n",
    "        }\n",
    "        return x, labels\n",
    "\n",
    "    def _iter_data(self) -> Iterable[Tuple[np.ndarray, Dict[str, np.ndarray]]]:\n",
    "        \"\"\"Generator that yields one batch at a time.\"\"\"\n",
    "        index = np.arange(self.size())\n",
    "        rnd = np.random.RandomState(self.seed)\n",
    "\n",
    "        if self.shuffle:\n",
    "            rnd.shuffle(index)\n",
    "        for b in range(self.steps_per_epoch()):\n",
    "            start = b * self.batch_size\n",
    "            idx = index[start:(start + self.batch_size)]\n",
    "            yield self._get_data_batch(idx)\n",
    "\n",
    "        if not self.drop_last:\n",
    "            start = self.steps_per_epoch() * self.batch_size\n",
    "            idx = index[start:]\n",
    "            yield self._get_data_batch(idx)\n",
    "\n",
    "    def _get_shapes(self) -> Tuple[tf.TensorShape, Dict[str, tf.TensorShape]]:\n",
    "        \"\"\"Return shapes of data returned by `self._iter_data`.\"\"\"\n",
    "        batch_size = self.batch_size if self.drop_last else None\n",
    "        h, w = self.input_size\n",
    "        c = self.channel_number\n",
    "        images = tf.TensorShape([batch_size, h, w, c])\n",
    "\n",
    "        labels = {k: tf.TensorShape((batch_size,))\n",
    "                  for k in (\"label_event\", \"label_time\")}\n",
    "        labels[\"label_riskset\"] = tf.TensorShape((batch_size, batch_size))\n",
    "        return images, labels\n",
    "\n",
    "    def _get_dtypes(self) -> Tuple[tf.DType, Dict[str, tf.DType]]:\n",
    "        \"\"\"Return dtypes of data returned by `self._iter_data`.\"\"\"\n",
    "        labels = {\"label_event\": tf.int32,\n",
    "                  \"label_time\": tf.float32,\n",
    "                  \"label_riskset\": tf.bool}\n",
    "        return tf.float32, labels\n",
    "\n",
    "    def _make_dataset(self) -> tf.data.Dataset:\n",
    "        \"\"\"Create dataset from generator.\"\"\"\n",
    "        options = tf.data.Options()\n",
    "        options.experimental_optimization.noop_elimination = True\n",
    "        # options.experimental_optimization.apply_default_optimizations=True\n",
    "        options.experimental_optimization.map_parallelization = True\n",
    "        ds = tf.data.Dataset.from_generator(\n",
    "            self._iter_data,\n",
    "            self._get_dtypes(),\n",
    "            self._get_shapes()\n",
    "        )\n",
    "        ds = ds.with_options(options)\n",
    "        if self.repeat > 1:\n",
    "            return ds.repeat(self.repeat)\n",
    "        else:\n",
    "            return ds\n",
    "\n",
    "    def __call__(self) -> tf.data.Dataset:\n",
    "        return self._make_dataset()\n",
    "\n",
    "\n",
    "def safe_normalize(x: tf.Tensor) -> tf.Tensor:\n",
    "    \"\"\"Normalize risk scores to avoid exp underflowing.\n",
    "\n",
    "    Note that only risk scores relative to each other matter.\n",
    "    If minimum risk score is negative, we shift scores so minimum\n",
    "    is at zero.\n",
    "    \"\"\"\n",
    "    x_min = tf.reduce_min(x, axis=0)\n",
    "    c = tf.zeros_like(x_min)\n",
    "    norm = tf.where(x_min < 0, -x_min, c)\n",
    "    return x + norm\n",
    "\n",
    "\n",
    "def logsumexp_masked(risk_scores: tf.Tensor,\n",
    "                     mask: tf.Tensor,\n",
    "                     axis: int = 0,\n",
    "                     keepdims: Optional[bool] = None) -> tf.Tensor:\n",
    "    \"\"\"Compute logsumexp across `axis` for entries where `mask` is true.\"\"\"\n",
    "    risk_scores.shape.assert_same_rank(mask.shape)\n",
    "\n",
    "    with tf.name_scope(\"logsumexp_masked\"):\n",
    "        mask_f = tf.cast(mask, risk_scores.dtype)\n",
    "        risk_scores_masked = tf.math.multiply(risk_scores, mask_f)\n",
    "        # for numerical stability, substract the maximum value\n",
    "        # before taking the exponential\n",
    "        amax = tf.reduce_max(risk_scores_masked, axis=axis, keepdims=True)\n",
    "        risk_scores_shift = risk_scores_masked - amax\n",
    "\n",
    "        exp_masked = tf.math.multiply(tf.exp(risk_scores_shift), mask_f)\n",
    "        exp_sum = tf.reduce_sum(exp_masked, axis=axis, keepdims=True)\n",
    "        output = amax + tf.math.log(exp_sum)\n",
    "        if not keepdims:\n",
    "            output = tf.squeeze(output, axis=axis)\n",
    "    return output\n",
    "\n",
    "\n",
    "class CoxPHLoss(tf.keras.losses.Loss):\n",
    "    \"\"\"Negative partial log-likelihood of Cox's proportional hazards model.\"\"\"\n",
    "\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "    def call(self,\n",
    "             y_true: Sequence[tf.Tensor],\n",
    "             y_pred: tf.Tensor) -> tf.Tensor:\n",
    "        \"\"\"Compute loss.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        y_true : list|tuple of tf.Tensor\n",
    "            The first element holds a binary vector where 1\n",
    "            indicates an event 0 censoring.\n",
    "            The second element holds the riskset, a\n",
    "            boolean matrix where the `i`-th row denotes the\n",
    "            risk set of the `i`-th instance, i.e. the indices `j`\n",
    "            for which the observer time `y_j >= y_i`.\n",
    "            Both must be rank 2 tensors.\n",
    "        y_pred : tf.Tensor\n",
    "            The predicted outputs. Must be a rank 2 tensor.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        loss : tf.Tensor\n",
    "            Loss for each instance in the batch.\n",
    "        \"\"\"\n",
    "        event, riskset = y_true\n",
    "        predictions = y_pred\n",
    "\n",
    "        pred_shape = predictions.shape\n",
    "        if pred_shape.ndims != 2:\n",
    "            raise ValueError(\"Rank mismatch: Rank of predictions (received %s) should \"\n",
    "                             \"be 2.\" % pred_shape.ndims)\n",
    "\n",
    "        if pred_shape[1] is None:\n",
    "            raise ValueError(\"Last dimension of predictions must be known.\")\n",
    "\n",
    "        if pred_shape[1] != 1:\n",
    "            raise ValueError(\"Dimension mismatch: Last dimension of predictions \"\n",
    "                             \"(received %s) must be 1.\" % pred_shape[1])\n",
    "\n",
    "        if event.shape.ndims != pred_shape.ndims:\n",
    "            raise ValueError(\"Rank mismatch: Rank of predictions (received %s) should \"\n",
    "                             \"equal rank of event (received %s)\" % (\n",
    "                                 pred_shape.ndims, event.shape.ndims))\n",
    "\n",
    "        if riskset.shape.ndims != 2:\n",
    "            raise ValueError(\"Rank mismatch: Rank of riskset (received %s) should \"\n",
    "                             \"be 2.\" % riskset.shape.ndims)\n",
    "\n",
    "        event = tf.cast(event, predictions.dtype)\n",
    "        predictions = safe_normalize(predictions)\n",
    "\n",
    "        with tf.name_scope(\"assertions\"):\n",
    "            assertions = (\n",
    "                tf.debugging.assert_less_equal(event, 1.),\n",
    "                tf.debugging.assert_greater_equal(event, 0.),\n",
    "                tf.debugging.assert_type(riskset, tf.bool)\n",
    "            )\n",
    "\n",
    "        # move batch dimension to the end so predictions get broadcast\n",
    "        # row-wise when multiplying by riskset\n",
    "        pred_t = tf.transpose(predictions)\n",
    "        # compute log of sum over risk set for each row\n",
    "        rr = logsumexp_masked(pred_t, riskset, axis=1, keepdims=True)\n",
    "        assert rr.shape.as_list() == predictions.shape.as_list()\n",
    "\n",
    "        losses = tf.math.multiply(event, rr - predictions)\n",
    "\n",
    "        return losses  # *0.00001\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CindexMetric:\n",
    "    \"\"\"Computes concordance index across one epoch.\"\"\"\n",
    "\n",
    "    def reset_states(self) -> None:\n",
    "        \"\"\"Clear the buffer of collected values.\"\"\"\n",
    "        self._data = {\n",
    "            \"label_time\": [],\n",
    "            \"label_event\": [],\n",
    "            \"prediction\": []\n",
    "        }\n",
    "\n",
    "    def update_state(self, y_true: Dict[str, tf.Tensor], y_pred: tf.Tensor) -> None:\n",
    "        \"\"\"Collect observed time, event indicator and predictions for a batch.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        y_true : dict\n",
    "            Must have two items:\n",
    "            `label_time`, a tensor containing observed time for one batch,\n",
    "            and `label_event`, a tensor containing event indicator for one batch.\n",
    "        y_pred : tf.Tensor\n",
    "            Tensor containing predicted risk score for one batch.\n",
    "        \"\"\"\n",
    "        self._data[\"label_time\"].append(y_true[\"label_time\"].numpy())\n",
    "        self._data[\"label_event\"].append(y_true[\"label_event\"].numpy())\n",
    "        self._data[\"prediction\"].append(tf.squeeze(y_pred).numpy())\n",
    "\n",
    "    def result(self) -> Dict[str, float]:\n",
    "        \"\"\"Computes the concordance index across collected values.\n",
    "\n",
    "        Returns\n",
    "        ----------\n",
    "        metrics : dict\n",
    "            Computed metrics.\n",
    "        \"\"\"\n",
    "        data = {}\n",
    "        for k, v in self._data.items():\n",
    "            data[k] = np.concatenate(v)\n",
    "        # (event_times, predicted_scores, event_observed=None)\n",
    "        results = concordance_index(\n",
    "            data[\"label_time\"],\n",
    "            data[\"prediction\"],\n",
    "            data[\"label_event\"] == 1)\n",
    "\n",
    "        result_data = {}\n",
    "        # names = (\"cindex\")#, \"concordant\", \"discordant\", \"tied_risk\")\n",
    "        # for k, v in zip(names, results):\n",
    "        result_data[\"cindex\"] = results\n",
    "\n",
    "        return result_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_train = np.load(\"./time_train_shuffled_10x.npy\")\n",
    "event_train = np.load(\"./event_train_shuffled_10x.npy\")\n",
    "image_train = np.load(\"./image_train_shuffled_10x.npy\", mmap_mode=\"r\")\n",
    "\n",
    "time_valid = np.load(\"./time_valid_shuffled_10x.npy\")\n",
    "event_valid = np.load(\"./event_valid_shuffled_10x.npy\")\n",
    "image_valid = np.load(\"./image_valid_shuffled_10x.npy\", mmap_mode=\"r\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_fn = InputFunction(image_train, time_train, event_train,\n",
    "                         drop_last=True,\n",
    "                         augmentation=True,\n",
    "                         repeat=1,\n",
    "                         shuffle=True,\n",
    "                         resize_img=False,\n",
    "                         input_size=(512, 512),\n",
    "                         batch_size=16)\n",
    "eval_fn = InputFunction(image_valid, time_valid, event=event_valid, resize_img=False,\n",
    "                        input_size=(512, 512))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from plexusnet.architecture import PlexusNet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.compat.v2.summary as summary\n",
    "import tensorflow_addons as tfa\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from tensorflow.python.ops import summary_ops_v2\n",
    "from tqdm import tqdm\n",
    "\n",
    "path_model = \"./PlexusNET_BCR_10x_COX\"\n",
    "if not os.path.exists(path_model):\n",
    "    os.mkdir(path_model)\n",
    "\n",
    "\n",
    "class TrainAndEvaluateModel:\n",
    "\n",
    "    def __init__(self, model, model_dir, train_dataset, eval_dataset,\n",
    "                 learning_rate, num_epochs, steps_per_epoch):\n",
    "        self.num_epochs = num_epochs\n",
    "        self.model_dir = model_dir\n",
    "\n",
    "        self.model = model\n",
    "\n",
    "        self.train_ds = train_dataset\n",
    "        self.val_ds = eval_dataset\n",
    "\n",
    "        self.optimizer = tfa.optimizers.MovingAverage(\n",
    "            tf.optimizers.Adam(learning_rate=learning_rate))\n",
    "        self.loss_fn = CoxPHLoss()\n",
    "\n",
    "        self.train_loss_metric = tf.keras.metrics.Mean(name=\"train_loss\")\n",
    "        self.val_loss_metric = tf.keras.metrics.Mean(name=\"val_loss\")\n",
    "        self.val_cindex_metric = CindexMetric()\n",
    "        self.steps_per_epoch = steps_per_epoch\n",
    "\n",
    "    @tf.function\n",
    "    def train_one_step(self, x, y_event, y_riskset):\n",
    "        y_event = tf.expand_dims(y_event, axis=1)\n",
    "        with tf.GradientTape() as tape:\n",
    "            logits = self.model(x, training=True)\n",
    "            train_loss = self.loss_fn(\n",
    "                y_true=[y_event, y_riskset], y_pred=logits)\n",
    "\n",
    "        with tf.name_scope(\"gradients\"):\n",
    "\n",
    "            grads = tape.gradient(train_loss, self.model.trainable_weights)\n",
    "            self.optimizer.apply_gradients(\n",
    "                zip(grads, self.model.trainable_weights))\n",
    "        return train_loss, logits\n",
    "\n",
    "    def train_and_evaluate(self):\n",
    "        ckpt = tf.train.Checkpoint(\n",
    "            step=tf.Variable(0, dtype=tf.int64),\n",
    "            optimizer=self.optimizer,\n",
    "            model=self.model)\n",
    "        ckpt_manager = tf.train.CheckpointManager(\n",
    "            ckpt, str(self.model_dir), max_to_keep=self.num_epochs)\n",
    "\n",
    "        if ckpt_manager.latest_checkpoint:\n",
    "            ckpt.restore(ckpt_manager.latest_checkpoint)\n",
    "            print(\n",
    "                f\"Latest checkpoint restored from {ckpt_manager.latest_checkpoint}.\")\n",
    "\n",
    "        train_summary_writer = summary.create_file_writer(\n",
    "            str(self.model_dir / \"train\"))\n",
    "        val_summary_writer = summary.create_file_writer(\n",
    "            str(self.model_dir / \"valid\"))\n",
    "\n",
    "        for epoch in range(self.num_epochs):\n",
    "            with train_summary_writer.as_default():\n",
    "                self.train_one_epoch(ckpt.step, epoch)\n",
    "            self.model.save(f\"{path_model}/model_{epoch}\")\n",
    "\n",
    "            # Run a validation loop at the end of each epoch.\n",
    "            with val_summary_writer.as_default():\n",
    "                v = self.evaluate(ckpt.step)\n",
    "\n",
    "        save_path = ckpt_manager.save()\n",
    "        print(f\"Saved checkpoint for step {ckpt.step.numpy()}: {save_path}\")\n",
    "\n",
    "    def train_one_epoch(self, step_counter, epoch):\n",
    "        progress = tqdm(self.train_ds, total=self.steps_per_epoch)\n",
    "        for x, y in progress:\n",
    "            train_loss, logits = self.train_one_step(\n",
    "                x, y[\"label_event\"], y[\"label_riskset\"])\n",
    "\n",
    "            step = int(step_counter)\n",
    "            if step == 0:\n",
    "                # see https://stackoverflow.com/questions/58843269/display-graph-using-tensorflow-v2-0-in-tensorboard\n",
    "                func = self.train_one_step.get_concrete_function(\n",
    "                    x, y[\"label_event\"], y[\"label_riskset\"])\n",
    "                summary_ops_v2.graph(func.graph)\n",
    "\n",
    "            # Update training metric.\n",
    "            self.train_loss_metric.update_state(train_loss)\n",
    "\n",
    "            # Log every 200 batches.\n",
    "            mean_loss = self.train_loss_metric.result()\n",
    "            progress.set_description_str(\n",
    "                desc=f\"Epoch: {1+epoch}/{self.num_epochs} | Loss: {mean_loss:.4f}\")\n",
    "            # save summaries\n",
    "            summary.scalar(\"loss\", mean_loss, step=step_counter)\n",
    "            # Reset training metrics\n",
    "            self.train_loss_metric.reset_states()\n",
    "\n",
    "            step_counter.assign_add(1)\n",
    "\n",
    "    @tf.function\n",
    "    def evaluate_one_step(self, x, y_event, y_riskset):\n",
    "        y_event = tf.expand_dims(y_event, axis=1)\n",
    "        val_logits = self.model(x, training=False)\n",
    "        val_loss = self.loss_fn(y_true=[y_event, y_riskset], y_pred=val_logits)\n",
    "        return val_loss, val_logits\n",
    "\n",
    "    def evaluate(self, step_counter):\n",
    "        self.val_cindex_metric.reset_states()\n",
    "\n",
    "        for x_val, y_val in self.val_ds:\n",
    "            val_loss, val_logits = self.evaluate_one_step(\n",
    "                x_val, y_val[\"label_event\"], y_val[\"label_riskset\"])\n",
    "\n",
    "            # Update val metrics\n",
    "            self.val_loss_metric.update_state(val_loss)\n",
    "            self.val_cindex_metric.update_state(y_val, val_logits)\n",
    "\n",
    "        val_loss = self.val_loss_metric.result()\n",
    "        summary.scalar(\"loss\",\n",
    "                       val_loss,\n",
    "                       step=step_counter)\n",
    "        self.val_loss_metric.reset_states()\n",
    "\n",
    "        val_cindex = self.val_cindex_metric.result()\n",
    "        for key, value in val_cindex.items():\n",
    "            summary.scalar(key, value, step=step_counter)\n",
    "        print(\n",
    "            f\"Validation: loss = {val_loss:.4f}, cindex = {val_cindex['cindex']:.4f}\")\n",
    "        return val_cindex['cindex']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm - rf ckpts-PlexusNET_BCR_10x_COX\n",
    "!mkdir ckpts-PlexusNET_BCR_10x_COX\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.backend.clear_session()  # Clears the state of the previous model\n",
    "model = PlexusNet(depth=5, length=2, junction=3, n_class=1, final_activation=\"linear\", initial_filter=6, filter_num_for_first_convlayer=4,\n",
    "                  input_shape=(512, 512), ApplyLayerNormalization=True, run_all_BN=False, type_of_block=\"soft_att\", GlobalPooling=\"avg\").model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_sh = tfa.optimizers.CyclicalLearningRate(initial_learning_rate=1e-6,\n",
    "                                            maximal_learning_rate=1e-3,\n",
    "                                            step_size=train_fn.steps_per_epoch()*4,\n",
    "                                            scale_fn=lambda x: 1.,\n",
    "                                            scale_mode=\"cycle\",\n",
    "                                            name=\"MyCyclicScheduler\")\n",
    "\n",
    "trainer = TrainAndEvaluateModel(\n",
    "    model=model,\n",
    "    model_dir=Path(\"./ckpts-PlexusNET_BCR_10x_COX\"),\n",
    "    train_dataset=train_fn(),\n",
    "    eval_dataset=eval_fn(),\n",
    "    learning_rate=lr_sh,\n",
    "    num_epochs=50,\n",
    "    steps_per_epoch=train_fn.steps_per_epoch()\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the TensorBoard notebook extension.\n",
    "%load_ext tensorboard\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train_and_evaluate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from plexusnet.architecture import LoadModel\n",
    "import PIL\n",
    "from tqdm import tqdm\n",
    "from collections import defaultdict\n",
    "valid_set = pd.read_csv(\"valid_set.csv\")\n",
    "valid_set = valid_set[valid_set.Filename.str.contains(\n",
    "    \"/B/\") == False]  # Exclude benign samples\n",
    "valid_set[\"X1st.BCR.Type\"].value_counts()\n",
    "valid_set[\"BCR_status\"] = 1-valid_set[\"X1st.BCR.Type\"].str.contains(\"-\")\n",
    "print(valid_set[\"X1st.BCR.Type\"].value_counts())\n",
    "print(valid_set[\"BCR_status\"].value_counts())\n",
    "\n",
    "\n",
    "def GetResult(results):\n",
    "    case_lst = defaultdict(list)\n",
    "    y_true_case = defaultdict(list)\n",
    "    Gls_case = defaultdict(list)\n",
    "    time_case = defaultdict(list)\n",
    "    for i, fl in enumerate(results):\n",
    "        case_id = list(results.keys())[i].split(\"-\")[2]\n",
    "        case_lst[case_id].extend(results[fl])\n",
    "        time_case[case_id].append(\n",
    "            valid_set[\"Interval.RP.to.BCR.or.last.contact.death\"].iloc[i])\n",
    "        y_true_case[case_id].append(valid_set.BCR_status.iloc[i])\n",
    "        Gls_case[case_id].append(list(results.keys())[i].split(\"/\")[2])\n",
    "    y_true_lst = []\n",
    "    y_pred_lst = []\n",
    "    y_time_lst = []\n",
    "    for key in y_true_case:\n",
    "        y_true_lst.append(y_true_case[key][0])\n",
    "        _m = np.mean(case_lst[key])\n",
    "        y_pred_lst.append(_m)\n",
    "        y_time_lst.append(time_case[key][0])\n",
    "    print(roc_auc_score(y_true_lst, y_pred_lst), concordance_index(\n",
    "        y_time_lst, 1-np.array(y_pred_lst), y_true_lst))\n",
    "    return {'roc': roc_auc_score(y_true_lst, y_pred_lst),\n",
    "            'cindex': concordance_index(y_time_lst, 1-np.array(y_pred_lst), y_true_lst)}\n",
    "\n",
    "\n",
    "def RunAnalyses(model_best):\n",
    "    results = defaultdict(list)\n",
    "    heatmaps = defaultdict(list)\n",
    "    for fl in tqdm(valid_set.Filename):\n",
    "        img = np.array(PIL.Image.open(fl).resize((5120//4, 5120//4)))\n",
    "        img = img[128:-128, 128:-128]\n",
    "        heatmap = np.zeros((3, 3), dtype=np.float)\n",
    "        patch = []\n",
    "        for j in range(0, img.shape[0]-256, 256):\n",
    "            for i in range(0, img.shape[1]-256, 256):\n",
    "                patch.append(img[j:j+512, i:i+512])\n",
    "        pr = model_best.predict(np.array(patch), verbose=0)\n",
    "        k = 0\n",
    "        for j in range(0, 3):\n",
    "            for i in range(0, 3):\n",
    "                heatmap[j, i] = pr[k]\n",
    "                k += 1\n",
    "        heatmaps[fl] = heatmap\n",
    "        results[fl] = pr\n",
    "    return heatmaps, results\n",
    "\n",
    "\n",
    "# Run Analyze and\n",
    "heatmaps_model = {}\n",
    "results_model = {}\n",
    "cindex_model = {}\n",
    "roc_model = {}\n",
    "\n",
    "for epoch in range(1, 50):\n",
    "    print(epoch)\n",
    "    model_best = LoadModel(f\"./PlexusNET_BCR_10x_COX/model_{epoch}\")\n",
    "    heatmaps, results = RunAnalyses(model_best)\n",
    "    v = GetResult(results)\n",
    "    heatmaps_model[epoch] = heatmaps\n",
    "    results_model[epoch] = results\n",
    "    cindex_model[epoch] = v[\"cindex\"]\n",
    "    roc_model[epoch] = v[\"roc\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch = 49\n",
    "model.load_weights(f\"./PlexusNET_BCR_10x_COX/model_{epoch:02d}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set = pd.read_csv(\"./test_set_OnlyTumor.csv\")\n",
    "development_set = pd.read_csv(\"./development_set_OnlyTumor.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_path = \"./dataset/\"\n",
    "\n",
    "\n",
    "def RunAnalyses(model_best, dataset):\n",
    "    results = defaultdict(list)\n",
    "    heatmaps = defaultdict(list)\n",
    "    for fl_ in tqdm(dataset.Filename):\n",
    "\n",
    "        fl = folder_path+fl_[1:]\n",
    "        # resize to 1280x1280 = ~10x\n",
    "        img = np.array(PIL.Image.open(fl).resize((5120//4, 5120//4)))\n",
    "        # reduce white area as the TMA core is centered.\n",
    "        img = img[128:-128, 128:-128]\n",
    "        heatmap = np.zeros((3, 3), dtype=np.float)\n",
    "        patch = []\n",
    "        for j in range(0, img.shape[0]-256, 256):\n",
    "            for i in range(0, img.shape[1]-256, 256):\n",
    "                img_C = np.array(PIL.Image.fromarray(\n",
    "                    img[j:j+512, i:i+512]).resize((512, 512)), dtype=np.uint8)\n",
    "                patch.append(img_C)\n",
    "        pr = model_best.predict(np.array(patch), verbose=0)\n",
    "        k = 0\n",
    "        for j in range(0, 3):\n",
    "            for i in range(0, 3):\n",
    "                heatmap[j, i] = pr[k]\n",
    "                k += 1\n",
    "        heatmaps[fl] = heatmap\n",
    "        results[fl] = pr\n",
    "    return heatmaps, results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, results_development_set = RunAnalyses(model, development_set)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, results_testset = RunAnalyses(model, test_set)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_development_set = defaultdict(list)\n",
    "for j in range(9):\n",
    "    for k in results_development_set:\n",
    "        columns_development_set[j].append(\n",
    "            results_development_set[k][j].flatten()[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_test = defaultdict(list)\n",
    "for j in range(9):\n",
    "    for k in results_testset:\n",
    "        columns_test[j].append(results_testset[k][j].flatten()[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in columns_test:\n",
    "    test_set[col] = columns_test[col]\n",
    "test_set.to_csv(f\"PlexusNet_COX_test_set.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in columns_development_set:\n",
    "    development_set[col] = columns_development_set[col]\n",
    "development_set.to_csv(f\"PlexusNet_COX_development_set.csv\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
